{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import DistilBertModel, DistilBertTokenizer, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, mean_squared_error, r2_score, precision_recall_curve\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imdb_data(data_dir):\n",
    "    data = []\n",
    "    sentiments = []\n",
    "    ratings = []\n",
    "    for label_type in ['pos', 'neg']:\n",
    "        dir_name = os.path.join(data_dir, label_type)\n",
    "        for fname in os.listdir(dir_name):\n",
    "            if fname.endswith(\".txt\"):\n",
    "                rating = int(re.search(r'_(\\d+)\\.txt', fname).group(1))\n",
    "                with open(os.path.join(dir_name, fname), 'r', encoding='utf-8') as f:\n",
    "                    data.append(f.read())\n",
    "                    sentiments.append(1 if label_type == 'pos' else 0)\n",
    "                    ratings.append(rating)\n",
    "    return pd.DataFrame({'review': data, 'sentiment': sentiments, 'rating': ratings})\n",
    "\n",
    "train_data = load_imdb_data('aclImdb/train')\n",
    "test_data = load_imdb_data('aclImdb/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bogdan\\Projects_bv\\film_rewiev\\film_review_venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def prepare_data(df, tokenizer, max_length=256):\n",
    "    encodings = tokenizer(df['review'].tolist(), truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
    "    return TensorDataset(\n",
    "        encodings['input_ids'], \n",
    "        encodings['attention_mask'], \n",
    "        torch.tensor(df['sentiment'].tolist(), dtype=torch.long),\n",
    "        torch.tensor(df['rating'].tolist(), dtype=torch.float)\n",
    "    )\n",
    "\n",
    "train_dataset = prepare_data(train_data, tokenizer)\n",
    "test_dataset = prepare_data(test_data, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 782/782 [05:31<00:00,  2.36it/s, loss=5.24]\n",
      "Epoch 2: 100%|██████████| 782/782 [05:36<00:00,  2.32it/s, loss=2.81]\n",
      "Epoch 3: 100%|██████████| 782/782 [05:35<00:00,  2.33it/s, loss=2.04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 782/782 [01:53<00:00,  6.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating metrics...\n",
      "\n",
      "Model Performance Metrics:\n",
      "sentiment_accuracy: 0.9101\n",
      "sentiment_precision: 0.9078\n",
      "sentiment_recall: 0.9130\n",
      "sentiment_f1: 0.9104\n",
      "rating_mse: 3.0796\n",
      "rating_rmse: 1.7549\n",
      "rating_r2: 0.7473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class DistilBertMultitaskModel(nn.Module):\n",
    "    def __init__(self, num_labels=2):\n",
    "        super(DistilBertMultitaskModel, self).__init__()\n",
    "        self.distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.sentiment_classifier = nn.Linear(self.distilbert.config.hidden_size, num_labels)\n",
    "        self.rating_regressor = nn.Linear(self.distilbert.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        sentiment_logits = self.sentiment_classifier(pooled_output)\n",
    "        rating = self.rating_regressor(pooled_output)\n",
    "        return sentiment_logits, rating\n",
    "\n",
    "model = DistilBertMultitaskModel()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "sentiment_loss_fn = nn.CrossEntropyLoss()\n",
    "rating_loss_fn = nn.MSELoss()\n",
    "\n",
    "def train(model, train_loader, optimizer, sentiment_loss_fn, rating_loss_fn, device, epochs=3):\n",
    "    model.train()\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch + 1}')\n",
    "        for batch in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids, attention_mask, sentiment_labels, rating_labels = [b.to(device) for b in batch]\n",
    "            sentiment_logits, rating_pred = model(input_ids, attention_mask)\n",
    "            sentiment_loss = sentiment_loss_fn(sentiment_logits, sentiment_labels)\n",
    "            rating_loss = rating_loss_fn(rating_pred.squeeze(), rating_labels)\n",
    "            loss = sentiment_loss + rating_loss\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            progress_bar.set_postfix({'loss': total_loss / (progress_bar.n + 1)})\n",
    "\n",
    "def evaluate(model, test_loader, device):\n",
    "    model.eval()\n",
    "    sentiment_predictions = []\n",
    "    rating_predictions = []\n",
    "    true_sentiments = []\n",
    "    true_ratings = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc='Evaluating'):\n",
    "            input_ids, attention_mask, sentiment_labels, rating_labels = [b.to(device) for b in batch]\n",
    "            sentiment_logits, rating_pred = model(input_ids, attention_mask)\n",
    "            sentiment_preds = torch.argmax(sentiment_logits, dim=1)\n",
    "            sentiment_predictions.extend(sentiment_preds.cpu().tolist())\n",
    "            rating_predictions.extend(rating_pred.squeeze().cpu().tolist())\n",
    "            true_sentiments.extend(sentiment_labels.cpu().tolist())\n",
    "            true_ratings.extend(rating_labels.cpu().tolist())\n",
    "    return sentiment_predictions, rating_predictions, true_sentiments, true_ratings\n",
    "\n",
    "def calculate_metrics(sentiment_predictions, rating_predictions, true_sentiments, true_ratings):\n",
    "    accuracy = accuracy_score(true_sentiments, sentiment_predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_sentiments, sentiment_predictions, average='binary')\n",
    "    \n",
    "    mse = mean_squared_error(true_ratings, rating_predictions)\n",
    "    rmse = mse ** 0.5\n",
    "    r2 = r2_score(true_ratings, rating_predictions)\n",
    "    \n",
    "    return {\n",
    "        'sentiment_accuracy': accuracy,\n",
    "        'sentiment_precision': precision,\n",
    "        'sentiment_recall': recall,\n",
    "        'sentiment_f1': f1,\n",
    "        'rating_mse': mse,\n",
    "        'rating_rmse': rmse,\n",
    "        'rating_r2': r2\n",
    "    }\n",
    "\n",
    "print(\"Training the model...\")\n",
    "train(model, train_loader, optimizer, sentiment_loss_fn, rating_loss_fn, device)\n",
    "\n",
    "print(\"\\nEvaluating the model...\")\n",
    "sentiment_predictions, rating_predictions, true_sentiments, true_ratings = evaluate(model, test_loader, device)\n",
    "\n",
    "print(\"\\nCalculating metrics...\")\n",
    "metrics = calculate_metrics(sentiment_predictions, rating_predictions, true_sentiments, true_ratings)\n",
    "\n",
    "print(\"\\nModel Performance Metrics:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('model/tokenizer\\\\tokenizer_config.json',\n",
       " 'model/tokenizer\\\\special_tokens_map.json',\n",
       " 'model/tokenizer\\\\vocab.txt',\n",
       " 'model/tokenizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'film_review_predict_distilbertbert.pth')\n",
    "tokenizer.save_pretrained('model/tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment_and_rating(text, model, tokenizer, device):\n",
    "    model.eval()\n",
    "    encoding = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=256)\n",
    "    encoding = {k: v.to(device) for k, v in encoding.items()}\n",
    "    with torch.no_grad():\n",
    "        sentiment_logits, rating_pred = model(**encoding)\n",
    "    sentiment_probs = torch.softmax(sentiment_logits, dim=1)\n",
    "    sentiment = \"Positive\" if sentiment_probs[0][1] > 0.5 else \"Negative\"\n",
    "    sentiment_confidence = sentiment_probs[0][1].item() if sentiment == \"Positive\" else sentiment_probs[0][0].item()\n",
    "    rating = rating_pred.squeeze().item()\n",
    "    return sentiment, sentiment_confidence, rating\n",
    "\n",
    "while True:\n",
    "    user_review = input(\"\\nEnter a movie review (or type 'quit' to exit): \")\n",
    "    if user_review.lower() == 'quit':\n",
    "        break\n",
    "    sentiment, confidence, rating = predict_sentiment_and_rating(user_review, model, tokenizer, device)\n",
    "    print(f\"Predicted sentiment: {sentiment}\")\n",
    "    print(f\"Sentiment confidence: {confidence:.2f}\")\n",
    "    print(f\"Predicted rating: {rating:.1f}/10\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
